{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "HJQMQHvxf8uM",
        "ExecuteTime": {
          "end_time": "2025-05-25T08:05:16.219449Z",
          "start_time": "2025-05-25T08:05:05.506718Z"
        }
      },
      "id": "HJQMQHvxf8uM",
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros(9, dtype=int)  # 0 = empty, 1 = agent, -1 = opponent\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "        return self.get_state()\n",
        "\n",
        "    def return_inverted_board(self):\n",
        "        return -self.board\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.board.copy()\n",
        "\n",
        "    def available_actions(self):\n",
        "        return [i for i in range(9) if self.board[i] == 0]\n",
        "\n",
        "    def check_winner(self):\n",
        "        combos = [(0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6)]\n",
        "        for a,b,c in combos:\n",
        "            s = self.board[a] + self.board[b] + self.board[c]\n",
        "            if s == 3:\n",
        "                return 1  # Agent wins\n",
        "            elif s == -3:\n",
        "                return -1  # Opponent wins\n",
        "        if 0 not in self.board:\n",
        "            return 0  # Draw\n",
        "        return None  # Game continues\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.board[action] != 0 or self.done:\n",
        "            return self.get_state(), -10000, True  # Invalid move penalty\n",
        "        self.board[action] = 1  # Agent move\n",
        "        winner = self.check_winner()\n",
        "        if winner is not None:\n",
        "            self.done = True\n",
        "            return self.get_state(), 1 if winner == 1 else 0, True\n",
        "        return self.get_state(), 0.5, False\n",
        "\n",
        "    def step_opp(self):\n",
        "        # Opponent move (random)\n",
        "        opp_actions = self.available_actions()\n",
        "        if opp_actions:\n",
        "            opp_action = random.choice(opp_actions)\n",
        "            self.board[opp_action] = -1\n",
        "        winner = self.check_winner()\n",
        "        if winner is not None:\n",
        "            self.done = True\n",
        "            return self.get_state(), -1 if winner == -1 else 0, True\n",
        "        return self.get_state(), 0, False\n",
        "\n",
        "    def step_opp_action(self,action):\n",
        "        self.board[action] = -1  # Agent move\n",
        "        winner = self.check_winner()\n",
        "        if winner is not None:\n",
        "            self.done = True\n",
        "            return self.get_state(), -1 if winner == -1 else 0, True\n",
        "        return self.get_state(), 0, False\n",
        "\n",
        "\n",
        "# --- DQN Model ---\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(9, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 9)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "-eqxNByrqNE2",
        "ExecuteTime": {
          "end_time": "2025-05-25T08:05:16.720620Z",
          "start_time": "2025-05-25T08:05:16.706590Z"
        }
      },
      "id": "-eqxNByrqNE2",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
        "        return (\n",
        "            torch.tensor(state, dtype=torch.float32),\n",
        "            torch.tensor(action),\n",
        "            torch.tensor(reward, dtype=torch.float32),\n",
        "            torch.tensor(next_state, dtype=torch.float32),\n",
        "            torch.tensor(done, dtype=torch.float32),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "_guQRxWTl3J3"
      },
      "id": "_guQRxWTl3J3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training ---\n",
        "def train(episodes = 1000):\n",
        "    env = TicTacToe()\n",
        "    model = DQN()\n",
        "    target_model = DQN()\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    buffer = ReplayBuffer()\n",
        "    batch_size = 64\n",
        "    gamma = 0.9999\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.1\n",
        "    epsilon_decay = 0.999\n",
        "    update_target_every = 20\n",
        "    total_reward = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice(env.available_actions())\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "                    # mask = torch.tensor([float('-inf')] * 9)\n",
        "                    # for a in env.available_actions():\n",
        "                        # mask[a] = 0\n",
        "                    action = torch.argmax(q_values).item()\n",
        "                    if action not in env.available_actions():\n",
        "                        buffer.push((state, action, -10, state, True))\n",
        "                        action = random.choice(env.available_actions())\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "            buffer.push((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            if not done:\n",
        "                _, _, done = env.step_opp()\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                s, a, r, s_, d = buffer.sample(batch_size)\n",
        "                q_values = model(s).gather(1, a.unsqueeze(1)).squeeze()\n",
        "                with torch.no_grad():\n",
        "                    q_next = target_model(s_).max(1)[0]\n",
        "                    q_target = r + gamma * q_next * (1 - d)\n",
        "                loss = nn.MSELoss()(q_values, q_target)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        if episode % update_target_every == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
        "            total_reward = 0\n",
        "\n",
        "    # torch.save(model.state_dict(), \"dqn_tictactoe.pth\")\n",
        "    print(\"Training complete.\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "IAkrptH2YXjT"
      },
      "id": "IAkrptH2YXjT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training ---\n",
        "def train_against_self(model, episodes = 1000):\n",
        "    env = TicTacToe()\n",
        "    target_model = DQN()\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    buffer = ReplayBuffer()\n",
        "    batch_size = 64\n",
        "    gamma = 0.999\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.1\n",
        "    epsilon_decay = 0.9999\n",
        "    update_target_every = 20\n",
        "    total_reward = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice(env.available_actions())\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "                    # mask = torch.tensor([float('-inf')] * 9)\n",
        "                    # for a in env.available_actions():\n",
        "                        # mask[a] = 0\n",
        "                    action = torch.argmax(q_values).item()\n",
        "                    if action not in env.available_actions():\n",
        "                        buffer.push((state, action, -10, state, True))\n",
        "                        action = random.choice(env.available_actions())\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "            buffer.push((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            if not done:\n",
        "                q_values = target_model(torch.tensor(-state, dtype=torch.float32))\n",
        "                mask = torch.tensor([float('-inf')] * 9)\n",
        "                for a in env.available_actions():\n",
        "                    mask[a] = 0\n",
        "                action_op = torch.argmax(q_values + mask).item()\n",
        "                _, _, done = env.step_opp_action(action_op)\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                s, a, r, s_, d = buffer.sample(batch_size)\n",
        "                q_values = model(s).gather(1, a.unsqueeze(1)).squeeze()\n",
        "                with torch.no_grad():\n",
        "                    q_next = target_model(s_).max(1)[0]\n",
        "                    q_target = r + gamma * q_next * (1 - d)\n",
        "                loss = nn.MSELoss()(q_values, q_target)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        if episode % update_target_every == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
        "            total_reward = 0\n",
        "\n",
        "    # torch.save(model.state_dict(), \"dqn_tictactoe.pth\")\n",
        "    print(\"Training complete.\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "JjimG5_-WHOQ"
      },
      "id": "JjimG5_-WHOQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_model = train(episodes=10000)\n",
        "dqn_model = train_against_self(dqn_model, episodes=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4z4GQZDX2Gs",
        "outputId": "bb22ae9c-a986-4ad4-da21-b4f1594cdad5"
      },
      "id": "s4z4GQZDX2Gs",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 1, Epsilon: 0.999\n",
            "Episode 100, Total Reward: 52, Epsilon: 0.904\n",
            "Episode 200, Total Reward: 50, Epsilon: 0.818\n",
            "Episode 300, Total Reward: 61, Epsilon: 0.740\n",
            "Episode 400, Total Reward: 61, Epsilon: 0.670\n",
            "Episode 500, Total Reward: 63, Epsilon: 0.606\n",
            "Episode 600, Total Reward: 60, Epsilon: 0.548\n",
            "Episode 700, Total Reward: 58, Epsilon: 0.496\n",
            "Episode 800, Total Reward: 62, Epsilon: 0.449\n",
            "Episode 900, Total Reward: 69, Epsilon: 0.406\n",
            "Episode 1000, Total Reward: 59, Epsilon: 0.367\n",
            "Episode 1100, Total Reward: 62, Epsilon: 0.332\n",
            "Episode 1200, Total Reward: 67, Epsilon: 0.301\n",
            "Episode 1300, Total Reward: 69, Epsilon: 0.272\n",
            "Episode 1400, Total Reward: 78, Epsilon: 0.246\n",
            "Episode 1500, Total Reward: 69, Epsilon: 0.223\n",
            "Episode 1600, Total Reward: 67, Epsilon: 0.202\n",
            "Episode 1700, Total Reward: 67, Epsilon: 0.182\n",
            "Episode 1800, Total Reward: 73, Epsilon: 0.165\n",
            "Episode 1900, Total Reward: 82, Epsilon: 0.149\n",
            "Episode 2000, Total Reward: 69, Epsilon: 0.135\n",
            "Episode 2100, Total Reward: 75, Epsilon: 0.122\n",
            "Episode 2200, Total Reward: 68, Epsilon: 0.111\n",
            "Episode 2300, Total Reward: 73, Epsilon: 0.100\n",
            "Episode 2400, Total Reward: 70, Epsilon: 0.100\n",
            "Episode 2500, Total Reward: 72, Epsilon: 0.100\n",
            "Episode 2600, Total Reward: 67, Epsilon: 0.100\n",
            "Episode 2700, Total Reward: 71, Epsilon: 0.100\n",
            "Episode 2800, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 2900, Total Reward: 69, Epsilon: 0.100\n",
            "Episode 3000, Total Reward: 71, Epsilon: 0.100\n",
            "Episode 3100, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 3200, Total Reward: 74, Epsilon: 0.100\n",
            "Episode 3300, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 3400, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 3500, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 3600, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 3700, Total Reward: 82, Epsilon: 0.100\n",
            "Episode 3800, Total Reward: 84, Epsilon: 0.100\n",
            "Episode 3900, Total Reward: 85, Epsilon: 0.100\n",
            "Episode 4000, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 4100, Total Reward: 83, Epsilon: 0.100\n",
            "Episode 4200, Total Reward: 82, Epsilon: 0.100\n",
            "Episode 4300, Total Reward: 85, Epsilon: 0.100\n",
            "Episode 4400, Total Reward: 84, Epsilon: 0.100\n",
            "Episode 4500, Total Reward: 90, Epsilon: 0.100\n",
            "Episode 4600, Total Reward: 93, Epsilon: 0.100\n",
            "Episode 4700, Total Reward: 82, Epsilon: 0.100\n",
            "Episode 4800, Total Reward: 78, Epsilon: 0.100\n",
            "Episode 4900, Total Reward: 94, Epsilon: 0.100\n",
            "Episode 5000, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 5100, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 5200, Total Reward: 78, Epsilon: 0.100\n",
            "Episode 5300, Total Reward: 85, Epsilon: 0.100\n",
            "Episode 5400, Total Reward: 79, Epsilon: 0.100\n",
            "Episode 5500, Total Reward: 68, Epsilon: 0.100\n",
            "Episode 5600, Total Reward: 78, Epsilon: 0.100\n",
            "Episode 5700, Total Reward: 75, Epsilon: 0.100\n",
            "Episode 5800, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 5900, Total Reward: 75, Epsilon: 0.100\n",
            "Episode 6000, Total Reward: 79, Epsilon: 0.100\n",
            "Episode 6100, Total Reward: 75, Epsilon: 0.100\n",
            "Episode 6200, Total Reward: 74, Epsilon: 0.100\n",
            "Episode 6300, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 6400, Total Reward: 79, Epsilon: 0.100\n",
            "Episode 6500, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 6600, Total Reward: 78, Epsilon: 0.100\n",
            "Episode 6700, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 6800, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 6900, Total Reward: 74, Epsilon: 0.100\n",
            "Episode 7000, Total Reward: 67, Epsilon: 0.100\n",
            "Episode 7100, Total Reward: 72, Epsilon: 0.100\n",
            "Episode 7200, Total Reward: 70, Epsilon: 0.100\n",
            "Episode 7300, Total Reward: 65, Epsilon: 0.100\n",
            "Episode 7400, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 7500, Total Reward: 84, Epsilon: 0.100\n",
            "Episode 7600, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 7700, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 7800, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 7900, Total Reward: 70, Epsilon: 0.100\n",
            "Episode 8000, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 8100, Total Reward: 83, Epsilon: 0.100\n",
            "Episode 8200, Total Reward: 86, Epsilon: 0.100\n",
            "Episode 8300, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 8400, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 8500, Total Reward: 74, Epsilon: 0.100\n",
            "Episode 8600, Total Reward: 75, Epsilon: 0.100\n",
            "Episode 8700, Total Reward: 87, Epsilon: 0.100\n",
            "Episode 8800, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 8900, Total Reward: 69, Epsilon: 0.100\n",
            "Episode 9000, Total Reward: 69, Epsilon: 0.100\n",
            "Episode 9100, Total Reward: 68, Epsilon: 0.100\n",
            "Episode 9200, Total Reward: 73, Epsilon: 0.100\n",
            "Episode 9300, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 9400, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 9500, Total Reward: 74, Epsilon: 0.100\n",
            "Episode 9600, Total Reward: 79, Epsilon: 0.100\n",
            "Episode 9700, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 9800, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 9900, Total Reward: 75, Epsilon: 0.100\n",
            "Training complete.\n",
            "Episode 0, Total Reward: 0, Epsilon: 1.000\n",
            "Episode 100, Total Reward: 39, Epsilon: 0.990\n",
            "Episode 200, Total Reward: 64, Epsilon: 0.980\n",
            "Episode 300, Total Reward: 75, Epsilon: 0.970\n",
            "Episode 400, Total Reward: 69, Epsilon: 0.961\n",
            "Episode 500, Total Reward: 71, Epsilon: 0.951\n",
            "Episode 600, Total Reward: 70, Epsilon: 0.942\n",
            "Episode 700, Total Reward: 72, Epsilon: 0.932\n",
            "Episode 800, Total Reward: 71, Epsilon: 0.923\n",
            "Episode 900, Total Reward: 76, Epsilon: 0.914\n",
            "Episode 1000, Total Reward: 80, Epsilon: 0.905\n",
            "Episode 1100, Total Reward: 80, Epsilon: 0.896\n",
            "Episode 1200, Total Reward: 89, Epsilon: 0.887\n",
            "Episode 1300, Total Reward: 84, Epsilon: 0.878\n",
            "Episode 1400, Total Reward: 86, Epsilon: 0.869\n",
            "Episode 1500, Total Reward: 80, Epsilon: 0.861\n",
            "Episode 1600, Total Reward: 88, Epsilon: 0.852\n",
            "Episode 1700, Total Reward: 84, Epsilon: 0.844\n",
            "Episode 1800, Total Reward: 79, Epsilon: 0.835\n",
            "Episode 1900, Total Reward: 81, Epsilon: 0.827\n",
            "Episode 2000, Total Reward: 78, Epsilon: 0.819\n",
            "Episode 2100, Total Reward: 84, Epsilon: 0.810\n",
            "Episode 2200, Total Reward: 70, Epsilon: 0.802\n",
            "Episode 2300, Total Reward: 72, Epsilon: 0.794\n",
            "Episode 2400, Total Reward: 70, Epsilon: 0.787\n",
            "Episode 2500, Total Reward: 77, Epsilon: 0.779\n",
            "Episode 2600, Total Reward: 74, Epsilon: 0.771\n",
            "Episode 2700, Total Reward: 74, Epsilon: 0.763\n",
            "Episode 2800, Total Reward: 84, Epsilon: 0.756\n",
            "Episode 2900, Total Reward: 66, Epsilon: 0.748\n",
            "Episode 3000, Total Reward: 69, Epsilon: 0.741\n",
            "Episode 3100, Total Reward: 68, Epsilon: 0.733\n",
            "Episode 3200, Total Reward: 71, Epsilon: 0.726\n",
            "Episode 3300, Total Reward: 74, Epsilon: 0.719\n",
            "Episode 3400, Total Reward: 75, Epsilon: 0.712\n",
            "Episode 3500, Total Reward: 78, Epsilon: 0.705\n",
            "Episode 3600, Total Reward: 70, Epsilon: 0.698\n",
            "Episode 3700, Total Reward: 58, Epsilon: 0.691\n",
            "Episode 3800, Total Reward: 64, Epsilon: 0.684\n",
            "Episode 3900, Total Reward: 51, Epsilon: 0.677\n",
            "Episode 4000, Total Reward: 65, Epsilon: 0.670\n",
            "Episode 4100, Total Reward: 68, Epsilon: 0.664\n",
            "Episode 4200, Total Reward: 54, Epsilon: 0.657\n",
            "Episode 4300, Total Reward: 60, Epsilon: 0.650\n",
            "Episode 4400, Total Reward: 60, Epsilon: 0.644\n",
            "Episode 4500, Total Reward: 64, Epsilon: 0.638\n",
            "Episode 4600, Total Reward: 52, Epsilon: 0.631\n",
            "Episode 4700, Total Reward: 54, Epsilon: 0.625\n",
            "Episode 4800, Total Reward: 53, Epsilon: 0.619\n",
            "Episode 4900, Total Reward: 56, Epsilon: 0.613\n",
            "Episode 5000, Total Reward: 66, Epsilon: 0.606\n",
            "Episode 5100, Total Reward: 62, Epsilon: 0.600\n",
            "Episode 5200, Total Reward: 63, Epsilon: 0.594\n",
            "Episode 5300, Total Reward: 66, Epsilon: 0.589\n",
            "Episode 5400, Total Reward: 65, Epsilon: 0.583\n",
            "Episode 5500, Total Reward: 67, Epsilon: 0.577\n",
            "Episode 5600, Total Reward: 58, Epsilon: 0.571\n",
            "Episode 5700, Total Reward: 49, Epsilon: 0.565\n",
            "Episode 5800, Total Reward: 48, Epsilon: 0.560\n",
            "Episode 5900, Total Reward: 48, Epsilon: 0.554\n",
            "Episode 6000, Total Reward: 60, Epsilon: 0.549\n",
            "Episode 6100, Total Reward: 47, Epsilon: 0.543\n",
            "Episode 6200, Total Reward: 43, Epsilon: 0.538\n",
            "Episode 6300, Total Reward: 42, Epsilon: 0.533\n",
            "Episode 6400, Total Reward: 45, Epsilon: 0.527\n",
            "Episode 6500, Total Reward: 54, Epsilon: 0.522\n",
            "Episode 6600, Total Reward: 46, Epsilon: 0.517\n",
            "Episode 6700, Total Reward: 54, Epsilon: 0.512\n",
            "Episode 6800, Total Reward: 41, Epsilon: 0.507\n",
            "Episode 6900, Total Reward: 48, Epsilon: 0.502\n",
            "Episode 7000, Total Reward: 47, Epsilon: 0.497\n",
            "Episode 7100, Total Reward: 41, Epsilon: 0.492\n",
            "Episode 7200, Total Reward: 46, Epsilon: 0.487\n",
            "Episode 7300, Total Reward: 56, Epsilon: 0.482\n",
            "Episode 7400, Total Reward: 41, Epsilon: 0.477\n",
            "Episode 7500, Total Reward: 52, Epsilon: 0.472\n",
            "Episode 7600, Total Reward: 46, Epsilon: 0.468\n",
            "Episode 7700, Total Reward: 50, Epsilon: 0.463\n",
            "Episode 7800, Total Reward: 48, Epsilon: 0.458\n",
            "Episode 7900, Total Reward: 41, Epsilon: 0.454\n",
            "Episode 8000, Total Reward: 44, Epsilon: 0.449\n",
            "Episode 8100, Total Reward: 39, Epsilon: 0.445\n",
            "Episode 8200, Total Reward: 50, Epsilon: 0.440\n",
            "Episode 8300, Total Reward: 49, Epsilon: 0.436\n",
            "Episode 8400, Total Reward: 54, Epsilon: 0.432\n",
            "Episode 8500, Total Reward: 54, Epsilon: 0.427\n",
            "Episode 8600, Total Reward: 46, Epsilon: 0.423\n",
            "Episode 8700, Total Reward: 55, Epsilon: 0.419\n",
            "Episode 8800, Total Reward: 43, Epsilon: 0.415\n",
            "Episode 8900, Total Reward: 59, Epsilon: 0.411\n",
            "Episode 9000, Total Reward: 53, Epsilon: 0.407\n",
            "Episode 9100, Total Reward: 66, Epsilon: 0.402\n",
            "Episode 9200, Total Reward: 66, Epsilon: 0.398\n",
            "Episode 9300, Total Reward: 69, Epsilon: 0.394\n",
            "Episode 9400, Total Reward: 46, Epsilon: 0.391\n",
            "Episode 9500, Total Reward: 66, Epsilon: 0.387\n",
            "Episode 9600, Total Reward: 59, Epsilon: 0.383\n",
            "Episode 9700, Total Reward: 69, Epsilon: 0.379\n",
            "Episode 9800, Total Reward: 49, Epsilon: 0.375\n",
            "Episode 9900, Total Reward: 54, Epsilon: 0.372\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_model = train_against_self(dqn_model, episodes=1000)\n",
        "dqn_model = train_against_self(dqn_model, episodes=1000)\n",
        "dqn_model = train_against_self(dqn_model, episodes=1000)\n",
        "dqn_model = train_against_self(dqn_model, episodes=1000)\n",
        "dqn_model = train_against_self(dqn_model, episodes=1000)"
      ],
      "metadata": {
        "id": "7hBeIZooZxYd",
        "outputId": "c5f5d46e-3882-4c19-b4d8-60cf920129ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7hBeIZooZxYd",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 0, Epsilon: 1.000\n",
            "Episode 100, Total Reward: 43, Epsilon: 0.990\n",
            "Episode 200, Total Reward: 51, Epsilon: 0.980\n",
            "Episode 300, Total Reward: 55, Epsilon: 0.970\n",
            "Episode 400, Total Reward: 50, Epsilon: 0.961\n",
            "Episode 500, Total Reward: 50, Epsilon: 0.951\n",
            "Episode 600, Total Reward: 52, Epsilon: 0.942\n",
            "Episode 700, Total Reward: 44, Epsilon: 0.932\n",
            "Episode 800, Total Reward: 55, Epsilon: 0.923\n",
            "Episode 900, Total Reward: 57, Epsilon: 0.914\n",
            "Training complete.\n",
            "Episode 0, Total Reward: 1, Epsilon: 1.000\n",
            "Episode 100, Total Reward: 48, Epsilon: 0.990\n",
            "Episode 200, Total Reward: 60, Epsilon: 0.980\n",
            "Episode 300, Total Reward: 55, Epsilon: 0.970\n",
            "Episode 400, Total Reward: 60, Epsilon: 0.961\n",
            "Episode 500, Total Reward: 56, Epsilon: 0.951\n",
            "Episode 600, Total Reward: 49, Epsilon: 0.942\n",
            "Episode 700, Total Reward: 59, Epsilon: 0.932\n",
            "Episode 800, Total Reward: 48, Epsilon: 0.923\n",
            "Episode 900, Total Reward: 63, Epsilon: 0.914\n",
            "Training complete.\n",
            "Episode 0, Total Reward: 0, Epsilon: 1.000\n",
            "Episode 100, Total Reward: 56, Epsilon: 0.990\n",
            "Episode 200, Total Reward: 62, Epsilon: 0.980\n",
            "Episode 300, Total Reward: 50, Epsilon: 0.970\n",
            "Episode 400, Total Reward: 50, Epsilon: 0.961\n",
            "Episode 500, Total Reward: 49, Epsilon: 0.951\n",
            "Episode 600, Total Reward: 55, Epsilon: 0.942\n",
            "Episode 700, Total Reward: 60, Epsilon: 0.932\n",
            "Episode 800, Total Reward: 54, Epsilon: 0.923\n",
            "Episode 900, Total Reward: 61, Epsilon: 0.914\n",
            "Training complete.\n",
            "Episode 0, Total Reward: 0, Epsilon: 1.000\n",
            "Episode 100, Total Reward: 65, Epsilon: 0.990\n",
            "Episode 200, Total Reward: 70, Epsilon: 0.980\n",
            "Episode 300, Total Reward: 61, Epsilon: 0.970\n",
            "Episode 400, Total Reward: 67, Epsilon: 0.961\n",
            "Episode 500, Total Reward: 67, Epsilon: 0.951\n",
            "Episode 600, Total Reward: 64, Epsilon: 0.942\n",
            "Episode 700, Total Reward: 67, Epsilon: 0.932\n",
            "Episode 800, Total Reward: 78, Epsilon: 0.923\n",
            "Episode 900, Total Reward: 86, Epsilon: 0.914\n",
            "Training complete.\n",
            "Episode 0, Total Reward: 1, Epsilon: 1.000\n",
            "Episode 100, Total Reward: 81, Epsilon: 0.990\n",
            "Episode 200, Total Reward: 75, Epsilon: 0.980\n",
            "Episode 300, Total Reward: 60, Epsilon: 0.970\n",
            "Episode 400, Total Reward: 57, Epsilon: 0.961\n",
            "Episode 500, Total Reward: 50, Epsilon: 0.951\n",
            "Episode 600, Total Reward: 60, Epsilon: 0.942\n",
            "Episode 700, Total Reward: 60, Epsilon: 0.932\n",
            "Episode 800, Total Reward: 63, Epsilon: 0.923\n",
            "Episode 900, Total Reward: 66, Epsilon: 0.914\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, games=100):\n",
        "    env = TicTacToe()\n",
        "    model.eval()\n",
        "\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "\n",
        "    for _ in range(games):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Agent move\n",
        "            with torch.no_grad():\n",
        "                q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "                mask = torch.tensor([float('-inf')] * 9)\n",
        "                for a in env.available_actions():\n",
        "                    mask[a] = 0\n",
        "                action = torch.argmax(q_values + mask).item()\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "            if not done:\n",
        "                next_state, reward, done = env.step_opp()\n",
        "            state = next_state\n",
        "\n",
        "        # Final result\n",
        "        if reward == 1:\n",
        "            wins += 1\n",
        "        elif reward == -1:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "    print(f\"Results over {games} games:\")\n",
        "    print(f\"Wins:   {wins}\")\n",
        "    print(f\"Losses: {losses}\")\n",
        "    print(f\"Draws:  {draws}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment to run evaluation\n",
        "    # train()\n",
        "    evaluate(model=dqn_model, games=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBW2fxGskGaG",
        "outputId": "8f442946-9bd7-453a-c4c8-96294c1f6d48"
      },
      "id": "qBW2fxGskGaG",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results over 100 games:\n",
            "Wins:   44\n",
            "Losses: 49\n",
            "Draws:  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_board(state):\n",
        "    symbols = {1: 'X', -1: 'O', 0: ' '}\n",
        "    board = [symbols[val] for val in state]\n",
        "    print(\"\\nBoard:\")\n",
        "    for i in range(3):\n",
        "        row = \" | \".join(board[i * 3:(i + 1) * 3])\n",
        "        print(row)\n",
        "        if i < 2:\n",
        "            print(\"---------\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "UTEiiTM9m4yQ"
      },
      "id": "UTEiiTM9m4yQ",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_vs_model(model_path=\"dqn_tictactoe.pth\", user_starts=True):\n",
        "    env = TicTacToe()\n",
        "    model = DQN()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    print(\"Welcome to Tic-Tac-Toe!\")\n",
        "    print(\"You are O (opponent), model is X (agent).\")\n",
        "    print(\"Board positions are 0-8:\")\n",
        "    print(\"0 | 1 | 2\\n3 | 4 | 5\\n6 | 7 | 8\\n\")\n",
        "\n",
        "    if not user_starts:\n",
        "        print(\"Model goes first.\")\n",
        "        with torch.no_grad():\n",
        "            q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "            mask = torch.tensor([float('-inf')] * 9)\n",
        "            for a in env.available_actions():\n",
        "                mask[a] = 0\n",
        "            action = torch.argmax(q_values + mask).item()\n",
        "        state, _, done = env.step(action)\n",
        "        print_board(state)\n",
        "\n",
        "    while not done:\n",
        "        # --- User move ---\n",
        "        user_action = -1\n",
        "        valid = env.available_actions()\n",
        "        while user_action not in valid:\n",
        "            try:\n",
        "                user_action = int(input(f\"Your move (available: {valid}): \"))\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        # Apply user move\n",
        "        env.board[user_action] = -1\n",
        "        result = env.check_winner()\n",
        "        if result is not None:\n",
        "            done = True\n",
        "            print_board(env.get_state())\n",
        "            if result == -1:\n",
        "                print(\"You win!\")\n",
        "            elif result == 0:\n",
        "                print(\"It's a draw!\")\n",
        "            else:\n",
        "                print(\"Model wins!\")\n",
        "            return\n",
        "\n",
        "        # --- Model move ---\n",
        "        state = env.get_state()\n",
        "        with torch.no_grad():\n",
        "            q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "            mask = torch.tensor([float('-inf')] * 9)\n",
        "            for a in env.available_actions():\n",
        "                mask[a] = 0\n",
        "            action = torch.argmax(q_values + mask).item()\n",
        "\n",
        "        state, _, done = env.step(action)\n",
        "        print_board(state)\n",
        "\n",
        "        if done:\n",
        "            result = env.check_winner()\n",
        "            if result == 1:\n",
        "                print(\"Model wins!\")\n",
        "            elif result == -1:\n",
        "                print(\"You win!\")\n",
        "            else:\n",
        "                print(\"It's a draw!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # train()\n",
        "    # evaluate()\n",
        "    play_vs_model(user_starts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF1YB_urmk0J",
        "outputId": "187a111f-c7c0-4fb2-a833-d2e800067488"
      },
      "id": "rF1YB_urmk0J",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-d9feff1dc25f>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Tic-Tac-Toe!\n",
            "You are O (opponent), model is X (agent).\n",
            "Board positions are 0-8:\n",
            "0 | 1 | 2\n",
            "3 | 4 | 5\n",
            "6 | 7 | 8\n",
            "\n",
            "Your move (available: [0, 1, 2, 3, 4, 5, 6, 7, 8]): 0\n",
            "\n",
            "Board:\n",
            "O |   |  \n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "X |   |  \n",
            "\n",
            "Your move (available: [1, 2, 3, 4, 5, 7, 8]): 5\n",
            "\n",
            "Board:\n",
            "O |   |  \n",
            "---------\n",
            "X |   | O\n",
            "---------\n",
            "X |   |  \n",
            "\n",
            "Your move (available: [1, 2, 4, 7, 8]): 4\n",
            "\n",
            "Board:\n",
            "O |   |  \n",
            "---------\n",
            "X | O | O\n",
            "---------\n",
            "X | X |  \n",
            "\n",
            "Your move (available: [1, 2, 8]): 8\n",
            "\n",
            "Board:\n",
            "O |   |  \n",
            "---------\n",
            "X | O | O\n",
            "---------\n",
            "X | X | O\n",
            "\n",
            "You win!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:dlEnv]",
      "name": "conda-env-dlEnv-py",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}