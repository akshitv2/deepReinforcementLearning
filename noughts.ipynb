{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import copy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "HJQMQHvxf8uM"
      },
      "id": "HJQMQHvxf8uM",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 0 = empty, 1 = X, -1 = O\n",
        "        self.board = [0] * 9\n",
        "        self.current_player = 1  # 1 for X, -1 for O\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "\n",
        "    def render(self):\n",
        "        symbols = {1: 'X', -1: 'O', 0: ' '}\n",
        "        for i in range(3):\n",
        "            row = [symbols[self.board[j]] for j in range(i*3, (i+1)*3)]\n",
        "            print('|'.join(row))\n",
        "            if i < 2:\n",
        "                print('-'*5)\n",
        "\n",
        "    def get_valid_actions(self):\n",
        "        return [i for i, x in enumerate(self.board) if x == 0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done or self.board[action] != 0:\n",
        "            raise ValueError(\"Invalid move\")\n",
        "\n",
        "        self.board[action] = self.current_player\n",
        "        self._check_game_over()\n",
        "        reward = 0\n",
        "        if self.done:\n",
        "            if self.winner == self.current_player:\n",
        "                reward = 1\n",
        "            elif self.winner == 0:\n",
        "                reward = 0.5  # draw\n",
        "            else:\n",
        "                reward = -1\n",
        "\n",
        "        self.current_player *= -1  # Switch players\n",
        "        return self.board.copy(), reward, self.done\n",
        "\n",
        "    def _check_game_over(self):\n",
        "        winning_combinations = [\n",
        "            [0,1,2], [3,4,5], [6,7,8],  # Rows\n",
        "            [0,3,6], [1,4,7], [2,5,8],  # Columns\n",
        "            [0,4,8], [2,4,6]            # Diagonals\n",
        "        ]\n",
        "        for combo in winning_combinations:\n",
        "            total = self.board[combo[0]] + self.board[combo[1]] + self.board[combo[2]]\n",
        "            if total == 3:\n",
        "                self.winner = 1\n",
        "                self.done = True\n",
        "                return\n",
        "            elif total == -3:\n",
        "                self.winner = -1\n",
        "                self.done = True\n",
        "                return\n",
        "\n",
        "        if all(cell != 0 for cell in self.board):\n",
        "            self.winner = 0  # Draw\n",
        "            self.done = True\n"
      ],
      "metadata": {
        "id": "-eqxNByrqNE2"
      },
      "id": "-eqxNByrqNE2",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size=9, action_size=9, gamma=0.95, epsilon=1.0,\n",
        "                 epsilon_min=0.1, epsilon_decay=0.995, learning_rate=0.001, batch_size=64):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            layers.Input(shape=(self.state_size,)),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(self.action_size)\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
        "                      loss=tf.keras.losses.MeanSquaredError())\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state, valid_actions):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(valid_actions)\n",
        "\n",
        "        q_values = self.model.predict(np.array([state]), verbose=0)[0]\n",
        "        # Mask invalid actions\n",
        "        masked_q_values = np.full(self.action_size, -np.inf)\n",
        "        for a in valid_actions:\n",
        "            masked_q_values[a] = q_values[a]\n",
        "        return np.argmax(masked_q_values)\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = np.zeros((self.batch_size, self.state_size))\n",
        "        targets = np.zeros((self.batch_size, self.action_size))\n",
        "\n",
        "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
        "            states[i] = state\n",
        "            target = self.model.predict(np.array([state]), verbose=0)[0]\n",
        "            if done:\n",
        "                target[action] = reward\n",
        "            else:\n",
        "                next_q = self.target_model.predict(np.array([next_state]), verbose=0)[0]\n",
        "                target[action] = reward + self.gamma * np.max(next_q)\n",
        "            targets[i] = target\n",
        "\n",
        "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "fe8Svdrhn5jo"
      },
      "id": "fe8Svdrhn5jo",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TicTacToe()\n",
        "env.render()\n",
        "print(\"Valid actions:\", env.get_valid_actions())\n",
        "state, reward, done = env.step(0)  # Player X moves\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXUysvbCnLwy",
        "outputId": "b896896f-7bdb-4deb-86a2-5f4450f9fd35"
      },
      "id": "mXUysvbCnLwy",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | | \n",
            "-----\n",
            " | | \n",
            "-----\n",
            " | | \n",
            "Valid actions: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
            "X| | \n",
            "-----\n",
            " | | \n",
            "-----\n",
            " | | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def train_dqn(episodes=1000, target_update_freq=10, save_dir=\"/content/saved_models\"):\n",
        "    env = TicTacToe()\n",
        "    agent = DQNAgent()\n",
        "    update_target_counter = 0\n",
        "\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    print(\"Training started...\")\n",
        "\n",
        "    for episode in tqdm(range(1, episodes + 1)):\n",
        "        env.reset()\n",
        "        state = np.array(env.board, dtype=np.float32)\n",
        "        total_reward = 0\n",
        "\n",
        "        while not env.done:\n",
        "            valid_actions = env.get_valid_actions()\n",
        "\n",
        "            if env.current_player == 1:\n",
        "                action = agent.act(state, valid_actions)\n",
        "                next_board, reward, done = env.step(action)\n",
        "                next_state = np.array(next_board, dtype=np.float32)\n",
        "                agent.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "            else:\n",
        "                action = random.choice(valid_actions)\n",
        "                env.step(action)\n",
        "                state = np.array(env.board, dtype=np.float32)\n",
        "\n",
        "        agent.replay()\n",
        "        update_target_counter += 1\n",
        "        if update_target_counter >= target_update_freq:\n",
        "            agent.update_target_model()\n",
        "            update_target_counter = 0\n",
        "\n",
        "        # Save model every 100 episodes\n",
        "        if episode % 100 == 0:\n",
        "            model_path = os.path.join(save_dir, f\"dqn_model_ep{episode}.keras\")\n",
        "            agent.model.save(model_path)\n",
        "            print(f\"Episode {episode}, Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f} — Model saved to {model_path}\")\n",
        "\n",
        "    return agent"
      ],
      "metadata": {
        "id": "hPKDaxugoDCd"
      },
      "id": "hPKDaxugoDCd",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_agent = train_dqn(episodes=1000, save_dir=\"/content/drive/MyDrive/Models/TicTacToe\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TViAz5-yoDtf",
        "outputId": "6610849e-0ebc-4183-95f0-2525fa5e209f"
      },
      "id": "TViAz5-yoDtf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 54/1000 [09:47<3:27:02, 13.13s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def play_game(agent):\n",
        "    env = TicTacToe()\n",
        "    env.reset()\n",
        "    state = np.array(env.board, dtype=np.float32)\n",
        "    env.render()\n",
        "\n",
        "    while not env.done:\n",
        "        if env.current_player == 1:\n",
        "            action = agent.act(state, env.get_valid_actions())\n",
        "        else:\n",
        "            action = random.choice(env.get_valid_actions())\n",
        "        env.step(action)\n",
        "        state = np.array(env.board, dtype=np.float32)\n",
        "        # print(\"\\nMove:\")\n",
        "        env.render()\n",
        "\n",
        "    result = \"Draw\" if env.winner == 0 else \"Agent wins!\" if env.winner == 1 else \"Opponent wins!\"\n",
        "    print(\"\\nGame Over:\", result)\n"
      ],
      "metadata": {
        "id": "Wxml0QrQoLaD"
      },
      "id": "Wxml0QrQoLaD",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_games(agent, num_games):\n",
        "    env = TicTacToe()\n",
        "    agent_wins = 0\n",
        "    opponent_wins = 0\n",
        "    draws = 0\n",
        "    for i in tqdm(range(num_games)):\n",
        "        env.reset()\n",
        "        state = np.array(env.board, dtype=np.float32)\n",
        "        # env.render()\n",
        "        while not env.done:\n",
        "            if env.current_player == 1:\n",
        "                action = agent.act(state, env.get_valid_actions())\n",
        "            else:\n",
        "                action = random.choice(env.get_valid_actions())\n",
        "            env.step(action)\n",
        "            state = np.array(env.board, dtype=np.float32)\n",
        "            # print(\"\\nMove:\")\n",
        "            # env.render()\n",
        "        # result = \"Draw\" if env.winner == 0 else \"Agent wins!\" if env.winner == 1 else \"Opponent wins!\"\n",
        "        if env.winner == 1:\n",
        "            agent_wins += 1\n",
        "        elif env.winner == -1:\n",
        "            opponent_wins += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "    print(\"\\nAgent wins: \", agent_wins/num_games, \"%\")\n",
        "    print(\"\\nOpponent wins: \", opponent_wins/num_games, \"%\")\n",
        "    print(\"Draws:\", draws/num_games, \"%\")"
      ],
      "metadata": {
        "id": "QpBNPi2JZwlp"
      },
      "id": "QpBNPi2JZwlp",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Path to your saved model file\n",
        "model_path = \"/content/drive/MyDrive/Models/TicTacToe/dqn_model_ep100.keras\"\n",
        "\n",
        "# Load the model\n",
        "loaded_model = load_model(model_path)\n",
        "loadedAgent = DQNAgent()\n",
        "loadedAgent.epsilon = 0.0\n",
        "loadedAgent.model = loaded_model"
      ],
      "metadata": {
        "id": "4084MdA_Jkp5"
      },
      "id": "4084MdA_Jkp5",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_games(loadedAgent,100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFOk9kNRZbzR",
        "outputId": "d1b521a0-bbff-414f-b5df-e0ca1c4ba1ef"
      },
      "id": "BFOk9kNRZbzR",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:53<00:00,  1.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Agent wins:  0.44 %\n",
            "\n",
            "Opponent wins:  0.41 %\n",
            "Draws: 0.15 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}