{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import copy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "HJQMQHvxf8uM"
      },
      "id": "HJQMQHvxf8uM",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 0 = empty, 1 = X, -1 = O\n",
        "        self.board = [0] * 9\n",
        "        self.current_player = 1  # 1 for X, -1 for O\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "\n",
        "    def render(self):\n",
        "        symbols = {1: 'X', -1: 'O', 0: ' '}\n",
        "        for i in range(3):\n",
        "            row = [symbols[self.board[j]] for j in range(i*3, (i+1)*3)]\n",
        "            print('|'.join(row))\n",
        "            if i < 2:\n",
        "                print('-'*5)\n",
        "\n",
        "    def get_valid_actions(self):\n",
        "        return [i for i, x in enumerate(self.board) if x == 0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done or self.board[action] != 0:\n",
        "            raise ValueError(\"Invalid move\")\n",
        "\n",
        "        self.board[action] = self.current_player\n",
        "        self._check_game_over()\n",
        "        reward = 0\n",
        "        if self.done:\n",
        "            if self.winner == self.current_player:\n",
        "                reward = 1\n",
        "            elif self.winner == 0:\n",
        "                reward = 0.5  # draw\n",
        "            else:\n",
        "                reward = -1\n",
        "\n",
        "        self.current_player *= -1  # Switch players\n",
        "        return self.board.copy(), reward, self.done\n",
        "\n",
        "    def _check_game_over(self):\n",
        "        winning_combinations = [\n",
        "            [0,1,2], [3,4,5], [6,7,8],  # Rows\n",
        "            [0,3,6], [1,4,7], [2,5,8],  # Columns\n",
        "            [0,4,8], [2,4,6]            # Diagonals\n",
        "        ]\n",
        "        for combo in winning_combinations:\n",
        "            total = self.board[combo[0]] + self.board[combo[1]] + self.board[combo[2]]\n",
        "            if total == 3:\n",
        "                self.winner = 1\n",
        "                self.done = True\n",
        "                return\n",
        "            elif total == -3:\n",
        "                self.winner = -1\n",
        "                self.done = True\n",
        "                return\n",
        "\n",
        "        if all(cell != 0 for cell in self.board):\n",
        "            self.winner = 0  # Draw\n",
        "            self.done = True\n"
      ],
      "metadata": {
        "id": "-eqxNByrqNE2"
      },
      "id": "-eqxNByrqNE2",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size=9, action_size=9, gamma=0.95, epsilon=1.0,\n",
        "                 epsilon_min=0.1, epsilon_decay=0.995, learning_rate=0.001, batch_size=64):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            layers.Input(shape=(self.state_size,)),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(self.action_size)\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
        "                      loss='mse')\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state, valid_actions):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(valid_actions)\n",
        "\n",
        "        q_values = self.model.predict(np.array([state]), verbose=0)[0]\n",
        "        # Mask invalid actions\n",
        "        masked_q_values = np.full(self.action_size, -np.inf)\n",
        "        for a in valid_actions:\n",
        "            masked_q_values[a] = q_values[a]\n",
        "        return np.argmax(masked_q_values)\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = np.zeros((self.batch_size, self.state_size))\n",
        "        targets = np.zeros((self.batch_size, self.action_size))\n",
        "\n",
        "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
        "            states[i] = state\n",
        "            target = self.model.predict(np.array([state]), verbose=0)[0]\n",
        "            if done:\n",
        "                target[action] = reward\n",
        "            else:\n",
        "                next_q = self.target_model.predict(np.array([next_state]), verbose=0)[0]\n",
        "                target[action] = reward + self.gamma * np.max(next_q)\n",
        "            targets[i] = target\n",
        "\n",
        "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "fe8Svdrhn5jo"
      },
      "id": "fe8Svdrhn5jo",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TicTacToe()\n",
        "env.render()\n",
        "print(\"Valid actions:\", env.get_valid_actions())\n",
        "state, reward, done = env.step(0)  # Player X moves\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXUysvbCnLwy",
        "outputId": "ab559013-05ae-4f08-a2a9-043e74c8059b"
      },
      "id": "mXUysvbCnLwy",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | | \n",
            "-----\n",
            " | | \n",
            "-----\n",
            " | | \n",
            "Valid actions: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
            "X| | \n",
            "-----\n",
            " | | \n",
            "-----\n",
            " | | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def train_dqn(episodes=1000, target_update_freq=10, save_dir=\"/content/saved_models\"):\n",
        "    env = TicTacToe()\n",
        "    agent = DQNAgent()\n",
        "    update_target_counter = 0\n",
        "\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    print(\"Training started...\")\n",
        "\n",
        "    for episode in tqdm(range(1, episodes + 1)):\n",
        "        env.reset()\n",
        "        state = np.array(env.board, dtype=np.float32)\n",
        "        total_reward = 0\n",
        "\n",
        "        while not env.done:\n",
        "            valid_actions = env.get_valid_actions()\n",
        "\n",
        "            if env.current_player == 1:\n",
        "                action = agent.act(state, valid_actions)\n",
        "                next_board, reward, done = env.step(action)\n",
        "                next_state = np.array(next_board, dtype=np.float32)\n",
        "                agent.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "            else:\n",
        "                action = random.choice(valid_actions)\n",
        "                env.step(action)\n",
        "                state = np.array(env.board, dtype=np.float32)\n",
        "\n",
        "        agent.replay()\n",
        "        update_target_counter += 1\n",
        "        if update_target_counter >= target_update_freq:\n",
        "            agent.update_target_model()\n",
        "            update_target_counter = 0\n",
        "\n",
        "        # Save model every 100 episodes\n",
        "        if episode % 100 == 0:\n",
        "            model_path = os.path.join(save_dir, f\"dqn_model_ep{episode}.h5\")\n",
        "            agent.model.save(model_path)\n",
        "            print(f\"Episode {episode}, Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f} — Model saved to {model_path}\")\n",
        "\n",
        "    return agent\n"
      ],
      "metadata": {
        "id": "hPKDaxugoDCd"
      },
      "id": "hPKDaxugoDCd",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iIkFrGE10sSa",
        "outputId": "8d585aa3-ab43-4427-ab3e-670fe59cdce5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "iIkFrGE10sSa",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_agent = train_dqn(episodes=1000, save_dir=\"/content/drive/MyDrive/Models/TicTacToe\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TViAz5-yoDtf",
        "outputId": "ca10d2a2-059a-469e-b424-cb922e77fc31"
      },
      "id": "TViAz5-yoDtf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 99/1000 [24:09<4:10:15, 16.66s/it]WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            " 10%|█         | 100/1000 [24:27<4:15:17, 17.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100, Reward: 0.50, Epsilon: 0.653 — Model saved to /content/drive/MyDrive/Models/TicTacToe/dqn_model_ep100.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 199/1000 [53:53<4:03:48, 18.26s/it]WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            " 20%|██        | 200/1000 [54:11<4:02:48, 18.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 200, Reward: 1.00, Epsilon: 0.396 — Model saved to /content/drive/MyDrive/Models/TicTacToe/dqn_model_ep200.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 299/1000 [1:24:21<3:37:17, 18.60s/it]WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            " 30%|███       | 300/1000 [1:24:38<3:33:27, 18.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 300, Reward: 0.50, Epsilon: 0.240 — Model saved to /content/drive/MyDrive/Models/TicTacToe/dqn_model_ep300.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 399/1000 [1:54:48<3:08:54, 18.86s/it]WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            " 40%|████      | 400/1000 [1:55:05<3:03:17, 18.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 400, Reward: 1.00, Epsilon: 0.145 — Model saved to /content/drive/MyDrive/Models/TicTacToe/dqn_model_ep400.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 499/1000 [2:25:38<2:28:31, 17.79s/it]WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            " 50%|█████     | 500/1000 [2:25:55<2:25:53, 17.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 500, Reward: 1.00, Epsilon: 0.100 — Model saved to /content/drive/MyDrive/Models/TicTacToe/dqn_model_ep500.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 506/1000 [2:27:46<2:34:07, 18.72s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def play_game(agent):\n",
        "    env = TicTacToe()\n",
        "    env.reset()\n",
        "    state = np.array(env.board, dtype=np.float32)\n",
        "    env.render()\n",
        "\n",
        "    while not env.done:\n",
        "        if env.current_player == 1:\n",
        "            action = agent.act(state, env.get_valid_actions())\n",
        "        else:\n",
        "            action = random.choice(env.get_valid_actions())\n",
        "        env.step(action)\n",
        "        state = np.array(env.board, dtype=np.float32)\n",
        "        print(\"\\nMove:\")\n",
        "        env.render()\n",
        "\n",
        "    result = \"Draw\" if env.winner == 0 else \"Agent wins!\" if env.winner == 1 else \"Opponent wins!\"\n",
        "    print(\"\\nGame Over:\", result)\n"
      ],
      "metadata": {
        "id": "Wxml0QrQoLaD"
      },
      "id": "Wxml0QrQoLaD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}