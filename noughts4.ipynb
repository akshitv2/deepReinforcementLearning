{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "HJQMQHvxf8uM",
        "ExecuteTime": {
          "end_time": "2025-05-25T08:05:16.219449Z",
          "start_time": "2025-05-25T08:05:05.506718Z"
        }
      },
      "id": "HJQMQHvxf8uM",
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros(9, dtype=int)  # 0 = empty, 1 = agent, -1 = opponent\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.board.copy()\n",
        "\n",
        "    def available_actions(self):\n",
        "        return [i for i in range(9) if self.board[i] == 0]\n",
        "\n",
        "    def check_winner(self):\n",
        "        combos = [(0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6)]\n",
        "        for a,b,c in combos:\n",
        "            s = self.board[a] + self.board[b] + self.board[c]\n",
        "            if s == 3:\n",
        "                return 1  # Agent wins\n",
        "            elif s == -3:\n",
        "                return -1  # Opponent wins\n",
        "        if 0 not in self.board:\n",
        "            return 0  # Draw\n",
        "        return None  # Game continues\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.board[action] != 0 or self.done:\n",
        "            return self.get_state(), -10, True  # Invalid move penalty\n",
        "        self.board[action] = 1  # Agent move\n",
        "        winner = self.check_winner()\n",
        "        if winner is not None:\n",
        "            self.done = True\n",
        "            return self.get_state(), 1 if winner == 1 else 0, True\n",
        "        return self.get_state(), 0, False\n",
        "\n",
        "    def step_opp(self):\n",
        "        # Opponent move (random)\n",
        "        opp_actions = self.available_actions()\n",
        "        if opp_actions:\n",
        "            opp_action = random.choice(opp_actions)\n",
        "            self.board[opp_action] = -1\n",
        "        winner = self.check_winner()\n",
        "        if winner is not None:\n",
        "            self.done = True\n",
        "            return self.get_state(), -1 if winner == -1 else 0, True\n",
        "        return self.get_state(), 0, False\n",
        "\n",
        "\n",
        "# --- DQN Model ---\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(9, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 9)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "-eqxNByrqNE2",
        "ExecuteTime": {
          "end_time": "2025-05-25T08:05:16.720620Z",
          "start_time": "2025-05-25T08:05:16.706590Z"
        }
      },
      "id": "-eqxNByrqNE2",
      "outputs": [],
      "execution_count": 40
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
        "        return (\n",
        "            torch.tensor(state, dtype=torch.float32),\n",
        "            torch.tensor(action),\n",
        "            torch.tensor(reward, dtype=torch.float32),\n",
        "            torch.tensor(next_state, dtype=torch.float32),\n",
        "            torch.tensor(done, dtype=torch.float32),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "_guQRxWTl3J3"
      },
      "id": "_guQRxWTl3J3",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training ---\n",
        "def train(episodes = 1000):\n",
        "    env = TicTacToe()\n",
        "    model = DQN()\n",
        "    target_model = DQN()\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    buffer = ReplayBuffer()\n",
        "    batch_size = 64\n",
        "    gamma = 0.99\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.1\n",
        "    epsilon_decay = 0.999\n",
        "    update_target_every = 20\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice(env.available_actions())\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "                    mask = torch.tensor([float('-inf')] * 9)\n",
        "                    for a in env.available_actions():\n",
        "                        mask[a] = 0\n",
        "                    action = torch.argmax(q_values + mask).item()\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "            buffer.push((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            if not done:\n",
        "                _, _, done = env.step_opp()\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                s, a, r, s_, d = buffer.sample(batch_size)\n",
        "                q_values = model(s).gather(1, a.unsqueeze(1)).squeeze()\n",
        "                with torch.no_grad():\n",
        "                    q_next = target_model(s_).max(1)[0]\n",
        "                    q_target = r + gamma * q_next * (1 - d)\n",
        "                loss = nn.MSELoss()(q_values, q_target)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        if episode % update_target_every == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"dqn_tictactoe.pth\")\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train(episodes=0)"
      ],
      "metadata": {
        "id": "IAkrptH2YXjT",
        "outputId": "55a4e52c-349b-4fab-e1f3-bc96dfb9eeb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IAkrptH2YXjT",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model_path=\"dqn_tictactoe.pth\", games=100):\n",
        "    env = TicTacToe()\n",
        "    model = DQN()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "\n",
        "    for _ in range(games):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Agent move\n",
        "            with torch.no_grad():\n",
        "                q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "                mask = torch.tensor([float('-inf')] * 9)\n",
        "                for a in env.available_actions():\n",
        "                    mask[a] = 0\n",
        "                action = torch.argmax(q_values + mask).item()\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "            if not done:\n",
        "                next_state, reward, done = env.step_opp()\n",
        "            state = next_state\n",
        "\n",
        "        # Final result\n",
        "        if reward == 1:\n",
        "            wins += 1\n",
        "        elif reward == -1:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "    print(f\"Results over {games} games:\")\n",
        "    print(f\"Wins:   {wins}\")\n",
        "    print(f\"Losses: {losses}\")\n",
        "    print(f\"Draws:  {draws}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment to run evaluation\n",
        "    # train()\n",
        "    evaluate(games=100)\n"
      ],
      "metadata": {
        "id": "qBW2fxGskGaG",
        "outputId": "73f9ae5e-ffe0-4e3d-e1f3-af4c3056d79d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qBW2fxGskGaG",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results over 100 games:\n",
            "Wins:   70\n",
            "Losses: 27\n",
            "Draws:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_board(state):\n",
        "    symbols = {1: 'X', -1: 'O', 0: ' '}\n",
        "    board = [symbols[val] for val in state]\n",
        "    print(\"\\nBoard:\")\n",
        "    for i in range(3):\n",
        "        row = \" | \".join(board[i * 3:(i + 1) * 3])\n",
        "        print(row)\n",
        "        if i < 2:\n",
        "            print(\"---------\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "UTEiiTM9m4yQ"
      },
      "id": "UTEiiTM9m4yQ",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_vs_model(model_path=\"dqn_tictactoe.pth\", user_starts=True):\n",
        "    env = TicTacToe()\n",
        "    model = DQN()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    print(\"Welcome to Tic-Tac-Toe!\")\n",
        "    print(\"You are O (opponent), model is X (agent).\")\n",
        "    print(\"Board positions are 0-8:\")\n",
        "    print(\"0 | 1 | 2\\n3 | 4 | 5\\n6 | 7 | 8\\n\")\n",
        "\n",
        "    if not user_starts:\n",
        "        print(\"Model goes first.\")\n",
        "        with torch.no_grad():\n",
        "            q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "            mask = torch.tensor([float('-inf')] * 9)\n",
        "            for a in env.available_actions():\n",
        "                mask[a] = 0\n",
        "            action = torch.argmax(q_values + mask).item()\n",
        "        state, _, done = env.step(action)\n",
        "        print_board(state)\n",
        "\n",
        "    while not done:\n",
        "        # --- User move ---\n",
        "        user_action = -1\n",
        "        valid = env.available_actions()\n",
        "        while user_action not in valid:\n",
        "            try:\n",
        "                user_action = int(input(f\"Your move (available: {valid}): \"))\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        # Apply user move\n",
        "        env.board[user_action] = -1\n",
        "        result = env.check_winner()\n",
        "        if result is not None:\n",
        "            done = True\n",
        "            print_board(env.get_state())\n",
        "            if result == -1:\n",
        "                print(\"You win!\")\n",
        "            elif result == 0:\n",
        "                print(\"It's a draw!\")\n",
        "            else:\n",
        "                print(\"Model wins!\")\n",
        "            return\n",
        "\n",
        "        # --- Model move ---\n",
        "        state = env.get_state()\n",
        "        with torch.no_grad():\n",
        "            q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "            mask = torch.tensor([float('-inf')] * 9)\n",
        "            for a in env.available_actions():\n",
        "                mask[a] = 0\n",
        "            action = torch.argmax(q_values + mask).item()\n",
        "\n",
        "        state, _, done = env.step(action)\n",
        "        print_board(state)\n",
        "\n",
        "        if done:\n",
        "            result = env.check_winner()\n",
        "            if result == 1:\n",
        "                print(\"Model wins!\")\n",
        "            elif result == -1:\n",
        "                print(\"You win!\")\n",
        "            else:\n",
        "                print(\"It's a draw!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # train()\n",
        "    # evaluate()\n",
        "    play_vs_model(user_starts=True)"
      ],
      "metadata": {
        "id": "rF1YB_urmk0J",
        "outputId": "97717fd4-faaf-4886-cb63-a29fd118e2a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rF1YB_urmk0J",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Tic-Tac-Toe!\n",
            "You are O (opponent), model is X (agent).\n",
            "Board positions are 0-8:\n",
            "0 | 1 | 2\n",
            "3 | 4 | 5\n",
            "6 | 7 | 8\n",
            "\n",
            "Your move (available: [0, 1, 2, 3, 4, 5, 6, 7, 8]): 0\n",
            "\n",
            "Board:\n",
            "O |   | X\n",
            "---------\n",
            "  |   |  \n",
            "---------\n",
            "  |   |  \n",
            "\n",
            "Your move (available: [1, 3, 4, 5, 6, 7, 8]): 4\n",
            "\n",
            "Board:\n",
            "O |   | X\n",
            "---------\n",
            "  | O |  \n",
            "---------\n",
            "  |   | X\n",
            "\n",
            "Your move (available: [1, 3, 5, 6, 7]): 5\n",
            "\n",
            "Board:\n",
            "O |   | X\n",
            "---------\n",
            "  | O | O\n",
            "---------\n",
            "  | X | X\n",
            "\n",
            "Your move (available: [1, 3, 6]): 6\n",
            "\n",
            "Board:\n",
            "O |   | X\n",
            "---------\n",
            "X | O | O\n",
            "---------\n",
            "O | X | X\n",
            "\n",
            "Your move (available: [1]): 1\n",
            "\n",
            "Board:\n",
            "O | O | X\n",
            "---------\n",
            "X | O | O\n",
            "---------\n",
            "O | X | X\n",
            "\n",
            "It's a draw!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:dlEnv]",
      "name": "conda-env-dlEnv-py",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}