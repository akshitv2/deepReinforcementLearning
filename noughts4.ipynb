{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "HJQMQHvxf8uM",
        "ExecuteTime": {
          "end_time": "2025-05-25T08:05:16.219449Z",
          "start_time": "2025-05-25T08:05:05.506718Z"
        }
      },
      "id": "HJQMQHvxf8uM",
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros(9, dtype=int)  # 0 = empty, 1 = agent, -1 = opponent\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "        return self.get_state()\n",
        "\n",
        "    def return_inverted_board(self):\n",
        "        return -self.board\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.board.copy()\n",
        "\n",
        "    def available_actions(self):\n",
        "        return [i for i in range(9) if self.board[i] == 0]\n",
        "\n",
        "    def check_winner(self):\n",
        "        combos = [(0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6)]\n",
        "        for a,b,c in combos:\n",
        "            s = self.board[a] + self.board[b] + self.board[c]\n",
        "            if s == 3:\n",
        "                return 1  # Agent wins\n",
        "            elif s == -3:\n",
        "                return -1  # Opponent wins\n",
        "        if 0 not in self.board:\n",
        "            return 0  # Draw\n",
        "        return None  # Game continues\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.board[action] != 0 or self.done:\n",
        "            return self.get_state(), -10, True  # Invalid move penalty\n",
        "        self.board[action] = 1  # Agent move\n",
        "        winner = self.check_winner()\n",
        "        if winner is not None:\n",
        "            self.done = True\n",
        "            return self.get_state(), 1 if winner == 1 else 0, True\n",
        "        return self.get_state(), 0, False\n",
        "\n",
        "    def step_opp(self):\n",
        "        # Opponent move (random)\n",
        "        opp_actions = self.available_actions()\n",
        "        if opp_actions:\n",
        "            opp_action = random.choice(opp_actions)\n",
        "            self.board[opp_action] = -1\n",
        "        winner = self.check_winner()\n",
        "        if winner is not None:\n",
        "            self.done = True\n",
        "            return self.get_state(), -1 if winner == -1 else 0, True\n",
        "        return self.get_state(), 0, False\n",
        "\n",
        "    def step_opp_action(self,action):\n",
        "        self.board[action] = -1  # Agent move\n",
        "        winner = self.check_winner()\n",
        "        if winner is not None:\n",
        "            self.done = True\n",
        "            return self.get_state(), -1 if winner == -1 else 0, True\n",
        "        return self.get_state(), 0, False\n",
        "\n",
        "\n",
        "# --- DQN Model ---\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(9, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 9)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "-eqxNByrqNE2",
        "ExecuteTime": {
          "end_time": "2025-05-25T08:05:16.720620Z",
          "start_time": "2025-05-25T08:05:16.706590Z"
        }
      },
      "id": "-eqxNByrqNE2",
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
        "        return (\n",
        "            torch.tensor(state, dtype=torch.float32),\n",
        "            torch.tensor(action),\n",
        "            torch.tensor(reward, dtype=torch.float32),\n",
        "            torch.tensor(next_state, dtype=torch.float32),\n",
        "            torch.tensor(done, dtype=torch.float32),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "_guQRxWTl3J3"
      },
      "id": "_guQRxWTl3J3",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training ---\n",
        "def train(episodes = 1000):\n",
        "    env = TicTacToe()\n",
        "    model = DQN()\n",
        "    target_model = DQN()\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    buffer = ReplayBuffer()\n",
        "    batch_size = 64\n",
        "    gamma = 0.9999\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.1\n",
        "    epsilon_decay = 0.999\n",
        "    update_target_every = 20\n",
        "    total_reward = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice(env.available_actions())\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "                    # mask = torch.tensor([float('-inf')] * 9)\n",
        "                    # for a in env.available_actions():\n",
        "                        # mask[a] = 0\n",
        "                    action = torch.argmax(q_values).item()\n",
        "                    if action not in env.available_actions():\n",
        "                        buffer.push((state, action, -10, state, True))\n",
        "                        action = random.choice(env.available_actions())\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "            buffer.push((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            if not done:\n",
        "                _, _, done = env.step_opp()\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                s, a, r, s_, d = buffer.sample(batch_size)\n",
        "                q_values = model(s).gather(1, a.unsqueeze(1)).squeeze()\n",
        "                with torch.no_grad():\n",
        "                    q_next = target_model(s_).max(1)[0]\n",
        "                    q_target = r + gamma * q_next * (1 - d)\n",
        "                loss = nn.MSELoss()(q_values, q_target)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        if episode % update_target_every == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
        "            total_reward = 0\n",
        "\n",
        "    # torch.save(model.state_dict(), \"dqn_tictactoe.pth\")\n",
        "    print(\"Training complete.\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "IAkrptH2YXjT"
      },
      "id": "IAkrptH2YXjT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training ---\n",
        "def train_against_self(model, episodes = 1000):\n",
        "    env = TicTacToe()\n",
        "    target_model = DQN()\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    buffer = ReplayBuffer()\n",
        "    batch_size = 64\n",
        "    gamma = 0.999\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.1\n",
        "    epsilon_decay = 0.9999\n",
        "    update_target_every = 20\n",
        "    total_reward = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice(env.available_actions())\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "                    # mask = torch.tensor([float('-inf')] * 9)\n",
        "                    # for a in env.available_actions():\n",
        "                        # mask[a] = 0\n",
        "                    action = torch.argmax(q_values).item()\n",
        "                    if action not in env.available_actions():\n",
        "                        buffer.push((state, action, -10, state, True))\n",
        "                        action = random.choice(env.available_actions())\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "            buffer.push((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            if not done:\n",
        "                q_values = model(torch.tensor(-state, dtype=torch.float32))\n",
        "                mask = torch.tensor([float('-inf')] * 9)\n",
        "                for a in env.available_actions():\n",
        "                    mask[a] = 0\n",
        "                action_op = torch.argmax(q_values + mask).item()\n",
        "                _, _, done = env.step_opp_action(action_op)\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                s, a, r, s_, d = buffer.sample(batch_size)\n",
        "                q_values = model(s).gather(1, a.unsqueeze(1)).squeeze()\n",
        "                with torch.no_grad():\n",
        "                    q_next = target_model(s_).max(1)[0]\n",
        "                    q_target = r + gamma * q_next * (1 - d)\n",
        "                loss = nn.MSELoss()(q_values, q_target)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        if episode % update_target_every == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
        "            total_reward = 0\n",
        "\n",
        "    # torch.save(model.state_dict(), \"dqn_tictactoe.pth\")\n",
        "    print(\"Training complete.\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "JjimG5_-WHOQ"
      },
      "id": "JjimG5_-WHOQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_model = train(episodes=10000)\n",
        "dqn_model = train_against_self(dqn_model, episodes=10000)"
      ],
      "metadata": {
        "id": "s4z4GQZDX2Gs",
        "outputId": "6a2d4f7e-abf4-4b43-ac6f-714c5877148b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "s4z4GQZDX2Gs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 1, Epsilon: 0.999\n",
            "Episode 100, Total Reward: 55, Epsilon: 0.904\n",
            "Episode 200, Total Reward: 63, Epsilon: 0.818\n",
            "Episode 300, Total Reward: 58, Epsilon: 0.740\n",
            "Episode 400, Total Reward: 56, Epsilon: 0.670\n",
            "Episode 500, Total Reward: 55, Epsilon: 0.606\n",
            "Episode 600, Total Reward: 60, Epsilon: 0.548\n",
            "Episode 700, Total Reward: 61, Epsilon: 0.496\n",
            "Episode 800, Total Reward: 57, Epsilon: 0.449\n",
            "Episode 900, Total Reward: 67, Epsilon: 0.406\n",
            "Episode 1000, Total Reward: 67, Epsilon: 0.367\n",
            "Episode 1100, Total Reward: 63, Epsilon: 0.332\n",
            "Episode 1200, Total Reward: 66, Epsilon: 0.301\n",
            "Episode 1300, Total Reward: 72, Epsilon: 0.272\n",
            "Episode 1400, Total Reward: 67, Epsilon: 0.246\n",
            "Episode 1500, Total Reward: 78, Epsilon: 0.223\n",
            "Episode 1600, Total Reward: 66, Epsilon: 0.202\n",
            "Episode 1700, Total Reward: 74, Epsilon: 0.182\n",
            "Episode 1800, Total Reward: 80, Epsilon: 0.165\n",
            "Episode 1900, Total Reward: 69, Epsilon: 0.149\n",
            "Episode 2000, Total Reward: 73, Epsilon: 0.135\n",
            "Episode 2100, Total Reward: 76, Epsilon: 0.122\n",
            "Episode 2200, Total Reward: 72, Epsilon: 0.111\n",
            "Episode 2300, Total Reward: 62, Epsilon: 0.100\n",
            "Episode 2400, Total Reward: 67, Epsilon: 0.100\n",
            "Episode 2500, Total Reward: 68, Epsilon: 0.100\n",
            "Episode 2600, Total Reward: 74, Epsilon: 0.100\n",
            "Episode 2700, Total Reward: 70, Epsilon: 0.100\n",
            "Episode 2800, Total Reward: 66, Epsilon: 0.100\n",
            "Episode 2900, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 3000, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 3100, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 3200, Total Reward: 84, Epsilon: 0.100\n",
            "Episode 3300, Total Reward: 78, Epsilon: 0.100\n",
            "Episode 3400, Total Reward: 82, Epsilon: 0.100\n",
            "Episode 3500, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 3600, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 3700, Total Reward: 79, Epsilon: 0.100\n",
            "Episode 3800, Total Reward: 79, Epsilon: 0.100\n",
            "Episode 3900, Total Reward: 75, Epsilon: 0.100\n",
            "Episode 4000, Total Reward: 84, Epsilon: 0.100\n",
            "Episode 4100, Total Reward: 82, Epsilon: 0.100\n",
            "Episode 4200, Total Reward: 83, Epsilon: 0.100\n",
            "Episode 4300, Total Reward: 86, Epsilon: 0.100\n",
            "Episode 4400, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 4500, Total Reward: 84, Epsilon: 0.100\n",
            "Episode 4600, Total Reward: 79, Epsilon: 0.100\n",
            "Episode 4700, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 4800, Total Reward: 85, Epsilon: 0.100\n",
            "Episode 4900, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 5000, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 5100, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 5200, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 5300, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 5400, Total Reward: 82, Epsilon: 0.100\n",
            "Episode 5500, Total Reward: 73, Epsilon: 0.100\n",
            "Episode 5600, Total Reward: 79, Epsilon: 0.100\n",
            "Episode 5700, Total Reward: 87, Epsilon: 0.100\n",
            "Episode 5800, Total Reward: 75, Epsilon: 0.100\n",
            "Episode 5900, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 6000, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 6100, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 6200, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 6300, Total Reward: 82, Epsilon: 0.100\n",
            "Episode 6400, Total Reward: 78, Epsilon: 0.100\n",
            "Episode 6500, Total Reward: 79, Epsilon: 0.100\n",
            "Episode 6600, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 6700, Total Reward: 85, Epsilon: 0.100\n",
            "Episode 6800, Total Reward: 90, Epsilon: 0.100\n",
            "Episode 6900, Total Reward: 84, Epsilon: 0.100\n",
            "Episode 7000, Total Reward: 67, Epsilon: 0.100\n",
            "Episode 7100, Total Reward: 68, Epsilon: 0.100\n",
            "Episode 7200, Total Reward: 67, Epsilon: 0.100\n",
            "Episode 7300, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 7400, Total Reward: 79, Epsilon: 0.100\n",
            "Episode 7500, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 7600, Total Reward: 82, Epsilon: 0.100\n",
            "Episode 7700, Total Reward: 78, Epsilon: 0.100\n",
            "Episode 7800, Total Reward: 83, Epsilon: 0.100\n",
            "Episode 7900, Total Reward: 82, Epsilon: 0.100\n",
            "Episode 8000, Total Reward: 82, Epsilon: 0.100\n",
            "Episode 8100, Total Reward: 85, Epsilon: 0.100\n",
            "Episode 8200, Total Reward: 84, Epsilon: 0.100\n",
            "Episode 8300, Total Reward: 86, Epsilon: 0.100\n",
            "Episode 8400, Total Reward: 71, Epsilon: 0.100\n",
            "Episode 8500, Total Reward: 83, Epsilon: 0.100\n",
            "Episode 8600, Total Reward: 88, Epsilon: 0.100\n",
            "Episode 8700, Total Reward: 85, Epsilon: 0.100\n",
            "Episode 8800, Total Reward: 81, Epsilon: 0.100\n",
            "Episode 8900, Total Reward: 83, Epsilon: 0.100\n",
            "Episode 9000, Total Reward: 82, Epsilon: 0.100\n",
            "Episode 9100, Total Reward: 65, Epsilon: 0.100\n",
            "Episode 9200, Total Reward: 73, Epsilon: 0.100\n",
            "Episode 9300, Total Reward: 78, Epsilon: 0.100\n",
            "Episode 9400, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 9500, Total Reward: 74, Epsilon: 0.100\n",
            "Episode 9600, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 9700, Total Reward: 74, Epsilon: 0.100\n",
            "Episode 9800, Total Reward: 75, Epsilon: 0.100\n",
            "Episode 9900, Total Reward: 80, Epsilon: 0.100\n",
            "Training complete.\n",
            "Episode 0, Total Reward: 0, Epsilon: 0.999\n",
            "Episode 100, Total Reward: 42, Epsilon: 0.904\n",
            "Episode 200, Total Reward: 56, Epsilon: 0.818\n",
            "Episode 300, Total Reward: 63, Epsilon: 0.740\n",
            "Episode 400, Total Reward: 67, Epsilon: 0.670\n",
            "Episode 500, Total Reward: 54, Epsilon: 0.606\n",
            "Episode 600, Total Reward: 62, Epsilon: 0.548\n",
            "Episode 700, Total Reward: 58, Epsilon: 0.496\n",
            "Episode 800, Total Reward: 70, Epsilon: 0.449\n",
            "Episode 900, Total Reward: 58, Epsilon: 0.406\n",
            "Episode 1000, Total Reward: 55, Epsilon: 0.367\n",
            "Episode 1100, Total Reward: 68, Epsilon: 0.332\n",
            "Episode 1200, Total Reward: 80, Epsilon: 0.301\n",
            "Episode 1300, Total Reward: 77, Epsilon: 0.272\n",
            "Episode 1400, Total Reward: 44, Epsilon: 0.246\n",
            "Episode 1500, Total Reward: 41, Epsilon: 0.223\n",
            "Episode 1600, Total Reward: 39, Epsilon: 0.202\n",
            "Episode 1700, Total Reward: 34, Epsilon: 0.182\n",
            "Episode 1800, Total Reward: 32, Epsilon: 0.165\n",
            "Episode 1900, Total Reward: 24, Epsilon: 0.149\n",
            "Episode 2000, Total Reward: 21, Epsilon: 0.135\n",
            "Episode 2100, Total Reward: 17, Epsilon: 0.122\n",
            "Episode 2200, Total Reward: 23, Epsilon: 0.111\n",
            "Episode 2300, Total Reward: 16, Epsilon: 0.100\n",
            "Episode 2400, Total Reward: 26, Epsilon: 0.100\n",
            "Episode 2500, Total Reward: 18, Epsilon: 0.100\n",
            "Episode 2600, Total Reward: 43, Epsilon: 0.100\n",
            "Episode 2700, Total Reward: 52, Epsilon: 0.100\n",
            "Episode 2800, Total Reward: 39, Epsilon: 0.100\n",
            "Episode 2900, Total Reward: 19, Epsilon: 0.100\n",
            "Episode 3000, Total Reward: 54, Epsilon: 0.100\n",
            "Episode 3100, Total Reward: 70, Epsilon: 0.100\n",
            "Episode 3200, Total Reward: 86, Epsilon: 0.100\n",
            "Episode 3300, Total Reward: 62, Epsilon: 0.100\n",
            "Episode 3400, Total Reward: 68, Epsilon: 0.100\n",
            "Episode 3500, Total Reward: 69, Epsilon: 0.100\n",
            "Episode 3600, Total Reward: 74, Epsilon: 0.100\n",
            "Episode 3700, Total Reward: 95, Epsilon: 0.100\n",
            "Episode 3800, Total Reward: 77, Epsilon: 0.100\n",
            "Episode 3900, Total Reward: 60, Epsilon: 0.100\n",
            "Episode 4000, Total Reward: 70, Epsilon: 0.100\n",
            "Episode 4100, Total Reward: 15, Epsilon: 0.100\n",
            "Episode 4200, Total Reward: 22, Epsilon: 0.100\n",
            "Episode 4300, Total Reward: 37, Epsilon: 0.100\n",
            "Episode 4400, Total Reward: 24, Epsilon: 0.100\n",
            "Episode 4500, Total Reward: 18, Epsilon: 0.100\n",
            "Episode 4600, Total Reward: 17, Epsilon: 0.100\n",
            "Episode 4700, Total Reward: 9, Epsilon: 0.100\n",
            "Episode 4800, Total Reward: 7, Epsilon: 0.100\n",
            "Episode 4900, Total Reward: 11, Epsilon: 0.100\n",
            "Episode 5000, Total Reward: 7, Epsilon: 0.100\n",
            "Episode 5100, Total Reward: 20, Epsilon: 0.100\n",
            "Episode 5200, Total Reward: 10, Epsilon: 0.100\n",
            "Episode 5300, Total Reward: 13, Epsilon: 0.100\n",
            "Episode 5400, Total Reward: 10, Epsilon: 0.100\n",
            "Episode 5500, Total Reward: 16, Epsilon: 0.100\n",
            "Episode 5600, Total Reward: 21, Epsilon: 0.100\n",
            "Episode 5700, Total Reward: 57, Epsilon: 0.100\n",
            "Episode 5800, Total Reward: 58, Epsilon: 0.100\n",
            "Episode 5900, Total Reward: 26, Epsilon: 0.100\n",
            "Episode 6000, Total Reward: 27, Epsilon: 0.100\n",
            "Episode 6100, Total Reward: 16, Epsilon: 0.100\n",
            "Episode 6200, Total Reward: 24, Epsilon: 0.100\n",
            "Episode 6300, Total Reward: 18, Epsilon: 0.100\n",
            "Episode 6400, Total Reward: 16, Epsilon: 0.100\n",
            "Episode 6500, Total Reward: 16, Epsilon: 0.100\n",
            "Episode 6600, Total Reward: 15, Epsilon: 0.100\n",
            "Episode 6700, Total Reward: 12, Epsilon: 0.100\n",
            "Episode 6800, Total Reward: 23, Epsilon: 0.100\n",
            "Episode 6900, Total Reward: 43, Epsilon: 0.100\n",
            "Episode 7000, Total Reward: 38, Epsilon: 0.100\n",
            "Episode 7100, Total Reward: 17, Epsilon: 0.100\n",
            "Episode 7200, Total Reward: 20, Epsilon: 0.100\n",
            "Episode 7300, Total Reward: 20, Epsilon: 0.100\n",
            "Episode 7400, Total Reward: 22, Epsilon: 0.100\n",
            "Episode 7500, Total Reward: 18, Epsilon: 0.100\n",
            "Episode 7600, Total Reward: 22, Epsilon: 0.100\n",
            "Episode 7700, Total Reward: 27, Epsilon: 0.100\n",
            "Episode 7800, Total Reward: 17, Epsilon: 0.100\n",
            "Episode 7900, Total Reward: 65, Epsilon: 0.100\n",
            "Episode 8000, Total Reward: 72, Epsilon: 0.100\n",
            "Episode 8100, Total Reward: 57, Epsilon: 0.100\n",
            "Episode 8200, Total Reward: 21, Epsilon: 0.100\n",
            "Episode 8300, Total Reward: 64, Epsilon: 0.100\n",
            "Episode 8400, Total Reward: 68, Epsilon: 0.100\n",
            "Episode 8500, Total Reward: 87, Epsilon: 0.100\n",
            "Episode 8600, Total Reward: 83, Epsilon: 0.100\n",
            "Episode 8700, Total Reward: 63, Epsilon: 0.100\n",
            "Episode 8800, Total Reward: 76, Epsilon: 0.100\n",
            "Episode 8900, Total Reward: 55, Epsilon: 0.100\n",
            "Episode 9000, Total Reward: 89, Epsilon: 0.100\n",
            "Episode 9100, Total Reward: 80, Epsilon: 0.100\n",
            "Episode 9200, Total Reward: 88, Epsilon: 0.100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, games=100):\n",
        "    env = TicTacToe()\n",
        "    model.eval()\n",
        "\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "\n",
        "    for _ in range(games):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Agent move\n",
        "            with torch.no_grad():\n",
        "                q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "                mask = torch.tensor([float('-inf')] * 9)\n",
        "                for a in env.available_actions():\n",
        "                    mask[a] = 0\n",
        "                action = torch.argmax(q_values + mask).item()\n",
        "\n",
        "            next_state, reward, done = env.step(action)\n",
        "            if not done:\n",
        "                next_state, reward, done = env.step_opp()\n",
        "            state = next_state\n",
        "\n",
        "        # Final result\n",
        "        if reward == 1:\n",
        "            wins += 1\n",
        "        elif reward == -1:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "    print(f\"Results over {games} games:\")\n",
        "    print(f\"Wins:   {wins}\")\n",
        "    print(f\"Losses: {losses}\")\n",
        "    print(f\"Draws:  {draws}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment to run evaluation\n",
        "    # train()\n",
        "    evaluate(model=dqn_model, games=100)\n"
      ],
      "metadata": {
        "id": "qBW2fxGskGaG"
      },
      "id": "qBW2fxGskGaG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_board(state):\n",
        "    symbols = {1: 'X', -1: 'O', 0: ' '}\n",
        "    board = [symbols[val] for val in state]\n",
        "    print(\"\\nBoard:\")\n",
        "    for i in range(3):\n",
        "        row = \" | \".join(board[i * 3:(i + 1) * 3])\n",
        "        print(row)\n",
        "        if i < 2:\n",
        "            print(\"---------\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "UTEiiTM9m4yQ"
      },
      "id": "UTEiiTM9m4yQ",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_vs_model(model_path=\"dqn_tictactoe.pth\", user_starts=True):\n",
        "    env = TicTacToe()\n",
        "    model = DQN()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    print(\"Welcome to Tic-Tac-Toe!\")\n",
        "    print(\"You are O (opponent), model is X (agent).\")\n",
        "    print(\"Board positions are 0-8:\")\n",
        "    print(\"0 | 1 | 2\\n3 | 4 | 5\\n6 | 7 | 8\\n\")\n",
        "\n",
        "    if not user_starts:\n",
        "        print(\"Model goes first.\")\n",
        "        with torch.no_grad():\n",
        "            q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "            mask = torch.tensor([float('-inf')] * 9)\n",
        "            for a in env.available_actions():\n",
        "                mask[a] = 0\n",
        "            action = torch.argmax(q_values + mask).item()\n",
        "        state, _, done = env.step(action)\n",
        "        print_board(state)\n",
        "\n",
        "    while not done:\n",
        "        # --- User move ---\n",
        "        user_action = -1\n",
        "        valid = env.available_actions()\n",
        "        while user_action not in valid:\n",
        "            try:\n",
        "                user_action = int(input(f\"Your move (available: {valid}): \"))\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        # Apply user move\n",
        "        env.board[user_action] = -1\n",
        "        result = env.check_winner()\n",
        "        if result is not None:\n",
        "            done = True\n",
        "            print_board(env.get_state())\n",
        "            if result == -1:\n",
        "                print(\"You win!\")\n",
        "            elif result == 0:\n",
        "                print(\"It's a draw!\")\n",
        "            else:\n",
        "                print(\"Model wins!\")\n",
        "            return\n",
        "\n",
        "        # --- Model move ---\n",
        "        state = env.get_state()\n",
        "        with torch.no_grad():\n",
        "            q_values = model(torch.tensor(state, dtype=torch.float32))\n",
        "            mask = torch.tensor([float('-inf')] * 9)\n",
        "            for a in env.available_actions():\n",
        "                mask[a] = 0\n",
        "            action = torch.argmax(q_values + mask).item()\n",
        "\n",
        "        state, _, done = env.step(action)\n",
        "        print_board(state)\n",
        "\n",
        "        if done:\n",
        "            result = env.check_winner()\n",
        "            if result == 1:\n",
        "                print(\"Model wins!\")\n",
        "            elif result == -1:\n",
        "                print(\"You win!\")\n",
        "            else:\n",
        "                print(\"It's a draw!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # train()\n",
        "    # evaluate()\n",
        "    play_vs_model(user_starts=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF1YB_urmk0J",
        "outputId": "2e007059-d7b6-4395-fba2-91648b4d59d5"
      },
      "id": "rF1YB_urmk0J",
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Tic-Tac-Toe!\n",
            "You are O (opponent), model is X (agent).\n",
            "Board positions are 0-8:\n",
            "0 | 1 | 2\n",
            "3 | 4 | 5\n",
            "6 | 7 | 8\n",
            "\n",
            "Your move (available: [0, 1, 2, 3, 4, 5, 6, 7, 8]): 0\n",
            "\n",
            "Board:\n",
            "O |   |  \n",
            "---------\n",
            "  | X |  \n",
            "---------\n",
            "  |   |  \n",
            "\n",
            "Your move (available: [1, 2, 3, 5, 6, 7, 8]): 8\n",
            "\n",
            "Board:\n",
            "O |   |  \n",
            "---------\n",
            "X | X |  \n",
            "---------\n",
            "  |   | O\n",
            "\n",
            "Your move (available: [1, 2, 5, 6, 7]): 5\n",
            "\n",
            "Board:\n",
            "O |   | X\n",
            "---------\n",
            "X | X | O\n",
            "---------\n",
            "  |   | O\n",
            "\n",
            "Your move (available: [1, 6, 7]): 6\n",
            "\n",
            "Board:\n",
            "O | X | X\n",
            "---------\n",
            "X | X | O\n",
            "---------\n",
            "O |   | O\n",
            "\n",
            "Your move (available: [7]): 7\n",
            "\n",
            "Board:\n",
            "O | X | X\n",
            "---------\n",
            "X | X | O\n",
            "---------\n",
            "O | O | O\n",
            "\n",
            "You win!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:dlEnv]",
      "name": "conda-env-dlEnv-py",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}