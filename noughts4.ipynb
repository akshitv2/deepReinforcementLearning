{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "id": "HJQMQHvxf8uM",
    "ExecuteTime": {
     "end_time": "2025-05-25T08:05:16.219449Z",
     "start_time": "2025-05-25T08:05:05.506718Z"
    }
   },
   "id": "HJQMQHvxf8uM",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "class checkBoard:\n",
    "    board = [[0, 0, 0],[0, 0, 0],[0, 0, 0]]\n",
    "    available_places = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "    def board_state(self):\n",
    "        return np.array(self.board).reshape(1, -1)\n",
    "\n",
    "    def check_winner(self, player):\n",
    "    # Check rows, columns and diagonals\n",
    "        for i in range(3):\n",
    "            if all([cell == player for cell in self.board[i]]) or \\\n",
    "            all([self.board[j][i] == player for j in range(3)]):\n",
    "                return True\n",
    "        if all([self.board[i][i] == player for i in range(3)]) or \\\n",
    "        all([self.board[i][2 - i] == player for i in range(3)]):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def game_end_state(self):\n",
    "        winner_one = self.check_winner(1)\n",
    "        winner_two = self.check_winner(2)\n",
    "        no_available_spaces = (len(self.available_places) == 0)\n",
    "        game_state = {}\n",
    "        game_state[\"winner\"] = 1 if winner_one else 2 if winner_two else 0\n",
    "        game_state[\"game_over\"] = no_available_spaces or winner_one or winner_two\n",
    "        game_state[\"tie\"] = no_available_spaces and not winner_one and not winner_two\n",
    "        game_state[\"reward_player_two\"] = -1 if winner_one else 1 if winner_two else 0.5\n",
    "        return game_state\n",
    "\n",
    "    def reset_board(self):\n",
    "        self.board = [[0, 0, 0],[0, 0, 0],[0, 0, 0]]\n",
    "        self.available_places = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "    def print_board(self):\n",
    "        for row in self.board:\n",
    "            print(\" | \".join(\"\".join(map(str, row)).replace(\"0\",\" \").replace(\"2\",\"X\").replace(\"1\",\"O\")))\n",
    "            print(\"-\" * 5)\n",
    "\n",
    "    def move(self, move, current_player = 2):\n",
    "        row, col = divmod(move, 3)\n",
    "        if self.board[row][col] > 0:\n",
    "            print(\"That spot is already taken. Try again.\")\n",
    "            return False\n",
    "        else:\n",
    "            self.board[row][col] = current_player\n",
    "            self.available_places.remove(move)\n",
    "            return True\n",
    "\n",
    "    def move_random(self, current_player = 1):\n",
    "        if not self.available_places:\n",
    "            return None\n",
    "        move_place = random.choice(self.available_places)\n",
    "        self.move(move_place, current_player)\n",
    "        return move_place"
   ],
   "metadata": {
    "id": "-eqxNByrqNE2",
    "ExecuteTime": {
     "end_time": "2025-05-25T08:05:16.720620Z",
     "start_time": "2025-05-25T08:05:16.706590Z"
    }
   },
   "id": "-eqxNByrqNE2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "NUM_ACTIONS = 9\n",
    "STATE_SIZE = 9\n",
    "EMPTY = 0\n",
    "PLAYER = 2\n",
    "OPPONENT = 1\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.1\n",
    "ALPHA = 0.001\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 10000"
   ],
   "metadata": {
    "id": "_guQRxWTl3J3"
   },
   "id": "_guQRxWTl3J3",
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_dqn_model(input_shape=(9,), num_actions=9):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_actions, activation='linear')  # Q-values for each action\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='mse')\n",
    "    return model\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((np.array(state).reshape(1, -1), action, reward, np.array(next_state).reshape(1, -1), done))\n",
    "\n",
    "    def sample(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        s, a, r, s_prime, done = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(s, dtype=torch.float),\n",
    "            torch.tensor(a),\n",
    "            torch.tensor(r),\n",
    "            torch.tensor(s_prime, dtype=torch.float),\n",
    "            torch.tensor(done, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class Agent:\n",
    "    dqn = create_dqn_model()\n",
    "    target_model = create_dqn_model()\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model"
   ],
   "metadata": {
    "id": "IAkrptH2YXjT"
   },
   "id": "IAkrptH2YXjT",
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def create_data(MEMORY_SIZE):\n",
    "    memory = deque(maxlen=MEMORY_SIZE)\n",
    "    for i in range(MEMORY_SIZE):\n",
    "        checkBoardGame = checkBoard()\n",
    "        checkBoardGame.reset_board()\n",
    "        game_state = checkBoardGame.game_end_state()\n",
    "        turn = True\n",
    "\n",
    "        while not(game_state[\"game_over\"]):\n",
    "            last_checkBoardState = copy.deepcopy(checkBoardGame.board_state()[0])\n",
    "            turn = not(turn)\n",
    "            move_made = None\n",
    "            if(turn):\n",
    "                move_made = checkBoardGame.move_random(2)\n",
    "            else:\n",
    "                checkBoardGame.move_random(1)\n",
    "            game_state = checkBoardGame.game_end_state()\n",
    "            if(move_made != None):\n",
    "                remember(memory, last_checkBoardState, move_made, game_state[\"reward_player_two\"], copy.deepcopy(checkBoardGame.board_state()[0]), game_state[\"game_over\"])\n",
    "\n",
    "    # checkBoardGame.print_board()\n",
    "    # print(game_state[\"winner\"])\n",
    "    return memory"
   ],
   "metadata": {
    "id": "Rqn8TJaIweox"
   },
   "id": "Rqn8TJaIweox",
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_dqn(model, memory, target_model):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(memory, BATCH_SIZE)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = np.array(states).reshape(BATCH_SIZE, -1)\n",
    "    next_states = np.array(next_states).reshape(BATCH_SIZE, -1)\n",
    "\n",
    "    targets = model.predict(states, verbose=0)\n",
    "    next_q = target_model.predict(next_states, verbose=0)\n",
    "\n",
    "    for i in range(BATCH_SIZE):\n",
    "        if dones[i]:\n",
    "            targets[i][actions[i]] = rewards[i] #Subtract 1 from actions[i] since it's using 1-based indexing\n",
    "        else:\n",
    "            valid_next_actions = [a for a in range(NUM_ACTIONS) if next_states[i][a] == EMPTY]\n",
    "            #Subtract 1 from the values in valid_next_actions\n",
    "            valid_next_actions = [a for a in valid_next_actions]\n",
    "            #Check if valid_next_actions is empty\n",
    "            if not valid_next_actions:\n",
    "                max_future_q = 0 # or any default value if there are no valid actions\n",
    "            else:\n",
    "                max_future_q = max(next_q[i][a] for a in valid_next_actions)\n",
    "            targets[i][actions[i]] = rewards[i] + GAMMA * max_future_q #Subtract 1 from actions[i]\n",
    "\n",
    "    model.fit(states, targets, epochs=1, verbose=0)"
   ],
   "metadata": {
    "id": "FJLckDilxnVl"
   },
   "id": "FJLckDilxnVl",
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def choose_action(model, board, epsilon=EPSILON):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.choice(board.available_places)\n",
    "\n",
    "    q_values = model.predict(board.board_state(), verbose=0)[0]\n",
    "    masked_q = np.full(NUM_ACTIONS, -np.inf)\n",
    "    for a in board.available_places:\n",
    "        masked_q[a] = q_values[a]\n",
    "    return np.argmax(masked_q)"
   ],
   "metadata": {
    "id": "EM8M0MAvhduB"
   },
   "id": "EM8M0MAvhduB",
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "73p60yzMA8SA"
   },
   "id": "73p60yzMA8SA"
  },
  {
   "cell_type": "code",
   "source": [
    "dqn_model = create_dqn_model()\n",
    "target_model = create_dqn_model()\n",
    "memory = create_data(MEMORY_SIZE)\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    train_dqn(dqn_model, memory, target_model)\n",
    "    target_model.set_weights(dqn_model.get_weights())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQTRL-aR96LX",
    "outputId": "aa6db3e3-2fb0-4973-dfe0-c2b83e1490b4"
   },
   "id": "oQTRL-aR96LX",
   "execution_count": 55,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100/100 [00:37<00:00,  2.69it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "checkBoardX = checkBoard()\n",
    "\n",
    "wins = [0,0,0]\n",
    "pbar = tqdm(range(1000))\n",
    "\n",
    "for i in pbar:\n",
    "    checkBoardX.reset_board()\n",
    "    turn = random.choice([True, False])\n",
    "    game_state = checkBoardX.game_end_state()\n",
    "    while not(game_state[\"game_over\"]):\n",
    "        turn = not(turn)\n",
    "        if(turn):\n",
    "            # print(checkBoardX.available_places)\n",
    "            move = choose_action(dqn_model, checkBoardX, 0)\n",
    "            # print(move)\n",
    "            checkBoardX.move(move, 2)\n",
    "            # checkBoardX.move_random(2)\n",
    "        else:\n",
    "            checkBoardX.move_random(1)\n",
    "        game_state = checkBoardX.game_end_state()\n",
    "    wins[game_state[\"winner\"]] += 1\n",
    "    pbar.set_postfix({'wins': wins})\n",
    "print(\"\\nPlayer 1 :\" , wins[1])\n",
    "print(\"Player 2 :\" , wins[2])\n",
    "print(\"Tie :\" , wins[0])"
   ],
   "metadata": {
    "id": "Odvc1kdYU_uV",
    "outputId": "0f57e68c-5910-4258-a33c-5d1f33116c2f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "Odvc1kdYU_uV",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  5%|▍         | 49/1000 [00:25<06:45,  2.34it/s, wins=[10, 25, 14]]"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlEnv]",
   "name": "conda-env-dlEnv-py",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
